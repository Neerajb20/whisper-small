{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import IPython.display as ipd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torchaudio\n",
    "import torch\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['id', 'sentence', 'prompt', 'speaker_id', 'path'],\n",
       " 'validation': ['id', 'sentence', 'prompt', 'speaker_id', 'path'],\n",
       " 'test': ['id', 'sentence', 'prompt', 'speaker_id', 'path']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"yashtiwari/PaulMooney-Medical-ASR-Data\")\n",
    "ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def resample_single_audio(sample):\n",
    "    # Extract the waveform and sample rate from the audio array\n",
    "    waveform = torch.tensor(sample[\"path\"][\"array\"],dtype=torch.float32).unsqueeze(0)  # Add a batch dimension\n",
    "    sample_rate = sample[\"path\"][\"sampling_rate\"]\n",
    "    \n",
    "    # Resample audio to 16kHz\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform_resampled = resampler(waveform)\n",
    "    \n",
    "    # Update the sample with the resampled audio\n",
    "    sample[\"path\"][\"array\"] = waveform_resampled.squeeze().numpy().tolist()  # Remove batch dimension and convert to list\n",
    "    sample[\"path\"][\"sampling_rate\"] = 16000\n",
    "    return sample\n",
    "\n",
    "# Apply resampling to a single audio sample\n",
    "resampled_audio = resample_single_audio(ds[\"test\"][200])\n",
    "print(resampled_audio[\"path\"][\"sampling_rate\"])  # Should print 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer, WhisperFeatureExtractor\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\",language=\"English\", task=\"transcribe\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# tokenizer.save_pretrained(\"./whisper-small-en\")\n",
    "# feature_extractor.save_pretrained(\"./whisper-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every morning when I wake up my neck feels like I slept on in wrong.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"./whisper-small-en\", language=\"English\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-small-en\").to(\"cuda:6\")\n",
    "\n",
    "# Remove unnecessary forced decoder ids setting (it's not needed for transcribing task)\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# Extract audio sample (ensure resampled_audio contains valid data)\n",
    "sample = resampled_audio[\"path\"]\n",
    "waveform = torch.tensor(sample[\"array\"], dtype=torch.float32)  # Ensure correct data type\n",
    "\n",
    "# Process audio to get input features\n",
    "inputs = processor(waveform.numpy(), sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Move input features to GPU\n",
    "input_features = inputs['input_features'].to(\"cuda:6\")  # Use 'input_features', not 'input_values'\n",
    "\n",
    "# Generate token IDs using input_features directly\n",
    "predicted_ids = model.generate(input_features=input_features)\n",
    "\n",
    "# Decode token IDs to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Every morning when I wake up my neck feels like I slept on in wrong.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cuda:6\")\n",
    "\n",
    "# Extract audio sample\n",
    "sample = resampled_audio[\"path\"]\n",
    "waveform = torch.tensor(sample[\"array\"], dtype=torch.float32)  # Ensure correct data type\n",
    "\n",
    "# Process audio\n",
    "input_features = processor(waveform.numpy(), sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "# Move input to GPU\n",
    "input_features = input_features.to(\"cuda:6\")\n",
    "# Generate token IDs\n",
    "predicted_ids = model.generate(input_features=input_features)\n",
    "\n",
    "# Decode token IDs to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "Processed 50 samples...\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Processed 100 samples...\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "Processed 150 samples...\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "Processed 200 samples...\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "Processed 250 samples...\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "Processed 300 samples...\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "Processed 350 samples...\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "\n",
      "Average WER for Fine-Tuned Whisper: 0.2759\n",
      "Average WER for OpenAI Whisper: 1.0857\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "# Load test dataset and WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def resample_single_audio(sample):\n",
    "    waveform = torch.tensor(sample[\"path\"][\"array\"], dtype=torch.float32).unsqueeze(0)\n",
    "    sample_rate = sample[\"path\"][\"sampling_rate\"]\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform_resampled = resampler(waveform)\n",
    "    sample[\"path\"][\"array\"] = waveform_resampled.squeeze().numpy().tolist()\n",
    "    sample[\"path\"][\"sampling_rate\"] = 16000\n",
    "    return sample\n",
    "\n",
    "def transcribe(model, processor, waveform, sampling_rate):\n",
    "    try:\n",
    "        # Preprocess the waveform to get input features\n",
    "        inputs = processor(\n",
    "            waveform.numpy(), \n",
    "            sampling_rate=sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_features = inputs['input_features']\n",
    "\n",
    "        # Check the shape and pad to length 3000 if needed\n",
    "        if input_features.shape[-1] < 3000:\n",
    "            pad_length = 3000 - input_features.shape[-1]\n",
    "            input_features = torch.nn.functional.pad(input_features, (0, pad_length), mode=\"constant\", value=-100)\n",
    "\n",
    "        # Move to GPU\n",
    "        input_features = input_features.to(\"cuda:6\")\n",
    "\n",
    "        # Generate transcription\n",
    "        predicted_ids = model.generate(input_features=input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        return transcription\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load Fine-Tuned Whisper Model\n",
    "processor_ft = WhisperProcessor.from_pretrained(\"./whisper-small-en\", language=\"English\", task=\"transcribe\")\n",
    "model_ft = WhisperForConditionalGeneration.from_pretrained(\"./whisper-small-en\").to(\"cuda:6\")\n",
    "model_ft.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# Load OpenAI Whisper Model\n",
    "processor_og = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"en\", task=\"transcribe\")\n",
    "model_og = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cuda:6\")\n",
    "\n",
    "# Initialize lists to store predictions and references\n",
    "predictions_ft = []\n",
    "predictions_og = []\n",
    "references = []\n",
    "\n",
    "# Loop through the entire test set\n",
    "for i, sample in enumerate(ds[\"validation\"]):\n",
    "    # Resample the audio sample\n",
    "    sample = resample_single_audio(sample)\n",
    "    waveform = torch.tensor(sample[\"path\"][\"array\"], dtype=torch.float32)\n",
    "    sampling_rate = sample[\"path\"][\"sampling_rate\"]\n",
    "    ground_truth = sample[\"sentence\"]\n",
    "    \n",
    "    try:\n",
    "        # Transcribe using Fine-Tuned Whisper\n",
    "        transcription_ft = transcribe(model_ft, processor_ft, waveform, sampling_rate)\n",
    "        \n",
    "        # Transcribe using OpenAI Whisper\n",
    "        transcription_og = transcribe(model_og, processor_og, waveform, sampling_rate)\n",
    "\n",
    "        # Append transcriptions and references if both are successful\n",
    "        if transcription_ft and transcription_og:\n",
    "            predictions_ft.append(transcription_ft)\n",
    "            predictions_og.append(transcription_og)\n",
    "            references.append(ground_truth)\n",
    "    \n",
    "        print(i)\n",
    "        # Progress print every 50 samples\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1} samples...\")\n",
    "\n",
    "        # Clear CUDA cache periodically to save memory\n",
    "        if (i + 1) % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {i} - {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate Average WER\n",
    "avg_wer_ft = wer_metric.compute(predictions=predictions_ft, references=references)\n",
    "avg_wer_og = wer_metric.compute(predictions=predictions_og, references=references)\n",
    "\n",
    "# Print the average WER scores\n",
    "print(f\"\\nAverage WER for Fine-Tuned Whisper: {avg_wer_ft:.4f}\")\n",
    "print(f\"Average WER for OpenAI Whisper: {avg_wer_og:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
