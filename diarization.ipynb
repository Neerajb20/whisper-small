{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:datasets:PyTorch version 2.6.0+cu118 available.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://67827e642aeb6b7db7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://67827e642aeb6b7db7.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://67827e642aeb6b7db7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/gradio/route_utils.py\", line 835, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/gradio/route_utils.py\", line 851, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/gradio/routes.py\", line 1475, in upload_file\n",
      "    form = await multipart_parser.parse()\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/gradio/route_utils.py\", line 689, in parse\n",
      "    async for chunk in self.stream:\n",
      "  File \"/mnt/nvme_disk2/User_data/nb57077k/miniconda3/envs/ax/lib/python3.10/site-packages/starlette/requests.py\", line 235, in stream\n",
      "    raise ClientDisconnect()\n",
      "starlette.requests.ClientDisconnect\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import gradio as gr\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "# Load Whisper\n",
    "processor = WhisperProcessor.from_pretrained(\"./whisper-small-en\", language=\"English\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-small-en\").to(\"cuda:6\")\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# Load diarization pipeline (no token needed for this model)\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
    "        use_auth_token=os.getenv(\"hugging_face_token\"))\n",
    "\n",
    "# Resample to 16kHz\n",
    "def resample_audio(waveform, orig_sr):\n",
    "    if orig_sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=16000)\n",
    "        waveform = resampler(torch.tensor(waveform))\n",
    "    return waveform.numpy(), 16000\n",
    "\n",
    "# Denoise audio\n",
    "def denoise(waveform, sample_rate):\n",
    "    noise_clip = waveform[:int(0.5 * sample_rate)]\n",
    "    return nr.reduce_noise(y=waveform, sr=sample_rate, y_noise=noise_clip)\n",
    "\n",
    "# Save waveform to temp WAV file for diarization\n",
    "def save_audio_temp(waveform, sample_rate):\n",
    "    tmpfile = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "    sf.write(tmpfile.name, waveform, sample_rate)\n",
    "    return tmpfile.name\n",
    "\n",
    "# Transcribe a waveform chunk using Whisper\n",
    "def transcribe_chunk(chunk_tensor, sample_rate=16000):\n",
    "    inputs = processor(chunk_tensor.numpy(), sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_features = inputs[\"input_features\"].to(\"cuda:6\")\n",
    "    predicted_ids = model.generate(input_features=input_features)\n",
    "    return processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Main diarized transcription function\n",
    "def diarized_transcribe(audio):\n",
    "    try:\n",
    "        sample_rate, waveform = audio\n",
    "\n",
    "        # Normalize if int16\n",
    "        if waveform.dtype == \"int16\":\n",
    "            waveform = waveform.astype(\"float32\") / 32768.0\n",
    "\n",
    "        # Denoise and resample\n",
    "        waveform = denoise(waveform, sample_rate)\n",
    "        waveform, sample_rate = resample_audio(waveform, sample_rate)\n",
    "        audio_path = save_audio_temp(waveform, sample_rate)\n",
    "\n",
    "        # Run diarization\n",
    "        diarization = pipeline(audio_path)\n",
    "        full_audio_tensor = torch.tensor(waveform)\n",
    "\n",
    "        # Build transcript\n",
    "        transcript = \"\"\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            start = int(turn.start * sample_rate)\n",
    "            end = int(turn.end * sample_rate)\n",
    "            segment = full_audio_tensor[start:end]\n",
    "\n",
    "            if segment.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            text = transcribe_chunk(segment)\n",
    "            transcript += f\"[{speaker} | {turn.start:.1f}s - {turn.end:.1f}s]: {text}\\n\"\n",
    "\n",
    "        return transcript.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during processing: {str(e)}\"\n",
    "\n",
    "# Gradio UI\n",
    "gr.Interface(\n",
    "    fn=diarized_transcribe,\n",
    "    inputs=gr.Audio(type=\"numpy\", label=\"Upload or Record Audio\"),\n",
    "    outputs=gr.Textbox(label=\"Speaker-attributed Transcription\"),\n",
    "    title=\"Whisper + Diarization\",\n",
    "    description=\"Denoises audio, diarizes speakers, and transcribes speech with Whisper.\",\n",
    "    allow_flagging=\"never\",\n",
    "    live=True\n",
    ").launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
